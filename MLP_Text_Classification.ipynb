{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db00378",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5db00378",
    "outputId": "51eddb7e-65c0-462c-cf79-65b2f1a020f4"
   },
   "outputs": [],
   "source": [
    "# !pip install portalocker\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OzgMJwpmc0iC",
   "metadata": {
    "id": "OzgMJwpmc0iC"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MgA6DK8R3BUU",
   "metadata": {
    "id": "MgA6DK8R3BUU"
   },
   "outputs": [],
   "source": [
    "# why optimizer.zero_grad()?\n",
    "\n",
    "# If we don't use optimizer.zero_grad(), gradients from previous steps don't get reset. This means they'll keep adding up.\n",
    "# For example, instead of following fresh footsteps in the snow each time, you keep walking in the old, deepening tracks.\n",
    "# If we only reset every 3 batches, we're combining the steps of those 3 batches.\n",
    "# It's like making a 3-step path in the snow before starting a new one. This might make our direction less precise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ab193",
   "metadata": {},
   "source": [
    "### Useful Information\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
    "- collate function: https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n",
    "- embedding layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5tQSyJ9cIOU",
   "metadata": {
    "id": "f5tQSyJ9cIOU"
   },
   "source": [
    "# Neural Text Classifier\n",
    "\n",
    "I will build a basic Neural Text Classifier. At a high level, the idea of this model goes as follows:\n",
    "\n",
    "- We are given a training set $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where each $x^{(i)}$ is a sentence and $y^{(i)}$ is a class label.\n",
    "- First, we need to loop over $\\{x^{(i)}\\}_{i=1}^{N}$ and get the Vocabulary, the number of unique words we see.\n",
    "- Once we do this, we will express each word as a one-hot representation. To do this, we will use a mapping from a unique word to an integer. For example, \"the\" might get index 3 and if there are 10 words (in the entire Vocabulary) then \"the\" would have a vector representation $x_{the} = (0,0,1,0,0,0,0,0,0,0)$. There will be many words in this Vocabulary, over 13,000. For this example, each word is mapped to a unique integer.\n",
    "- We will feed batches of data to the model and each batch will be transformed into a tensor with words each word transformed to its integer index in VOCAB below.\n",
    "- For example, we might get [[\"the man walks\"], [\"this is a sentence\"]] -> [[\"the\", \"man\", \"walks\"], [\"this\", \"is\", \"a\", \"sentence\"]] -> [[1, 4, 5], [6, 7, 8, 15]]. It depends on what unique integer each word gets.\n",
    "- Different sentences have different numbers of tokens but all batches need to be the same dimension (this is how PyTorch works), so we need a padding token. So, for example, if the batch size is B = 2 and we given two sentences like [\"a b c\", \"a b c d e\"] then as a tensor this will become [[1, 2, 3, 0, 0], [1, 2, 3, 4, 5]] and notice that we padded the first example so that the tensor is of dimension (2, 5) with M = 5. In some sense, in each batch we need to figure out the maximum number of tokens for an instance and pad each instance to have the same length as this longest instance. To do the above, use the [collate function](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders). The idea here is that the Dataloader takes in raw data and the collate function is applied to this data, returning formatting tensors we can use later on in the optimization. \n",
    "- After padding, we feed batches of data to the classifier, these are of dimension (B, M). For example, we have a batch size of 2 above and M = 5. This will depend on the batch but here the batch size is B.\n",
    "- Once we feed in (B, M) data to the network, we rewrite this as (B, M, vocab_size) by using a one-hot representation for each word.\n",
    "- Then, we first take an average agross all the M elements of each element of the batch to get a (B, vocab_size) tensor that represents each instance. We pass this tensor through linear layer and nonlinear layers as unusual. The model returns logits, without the Softmax applied. This is a multiclass classfication task.\n",
    "- Finally, we optimize the network and check it's train and validation set accuracies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9140e3c",
   "metadata": {
    "id": "b9140e3c"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9ace94f",
   "metadata": {
    "id": "c9ace94f"
   },
   "outputs": [],
   "source": [
    "# This is the dataset\n",
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "LR = 8.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MIN_FREQUENCY = 20\n",
    "# Padding valued used; if we have a tensor data x = [[1,2,3], [4, 5], [1,2,3,4,5]] this needs padding\n",
    "# As a tensor, this is t = [[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [1, 2, 3, 4, 5]]\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K3zuWGeqcDsI",
   "metadata": {
    "id": "K3zuWGeqcDsI"
   },
   "source": [
    "### Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f471ac",
   "metadata": {
    "id": "16f471ac"
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer by using get_tokenizer; pass \"basic_english\"\n",
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b61bba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1b61bba",
    "outputId": "b9810b51-a992-4026-f540-656402de8e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'some', 'text', '.', '.', '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_english_tokenizer(\"This is some text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68a50055",
   "metadata": {
    "id": "68a50055"
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer as a contant; this is needed later\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620a436",
   "metadata": {
    "id": "8620a436"
   },
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c84225a",
   "metadata": {
    "id": "1c84225a"
   },
   "outputs": [],
   "source": [
    "# Loop through all the (label, text) data and yield a tokenized version of text\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rEChT6jDeLXF",
   "metadata": {
    "id": "rEChT6jDeLXF"
   },
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "J4q-FQ75eM1R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4q-FQ75eM1R",
    "outputId": "1df76bd3-d2fc-4caa-aae7-bc5160838f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n"
     ]
    }
   ],
   "source": [
    "# An example of what this data looks like\n",
    "for y, x in train_iter:\n",
    "  print(y, x)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "affa3375",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affa3375",
    "outputId": "e6922daa-0b91-4f09-c340-8cd1a51bb7ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# Use build_vocab_from_iterator to get the the vocabulary\n",
    "# This is essentially a dictionary going from a word to a unique integer\n",
    "# Make sure to specify the specials\n",
    "VOCAB = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter),\n",
    "    min_freq = MIN_FREQUENCY,\n",
    "    specials=('<pad>', '<unk>')\n",
    ")\n",
    "\n",
    "# Set the default index to 1\n",
    "# Otherwise, VOCAB['unknownbigword'] will raise an Exception\n",
    "# I.e. we want '<unk>' to be the unknown word\n",
    "VOCAB.set_default_index(VOCAB['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62336be9",
   "metadata": {
    "id": "62336be9"
   },
   "outputs": [],
   "source": [
    "assert VOCAB['<unk>'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518918c0",
   "metadata": {
    "id": "518918c0"
   },
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de48bde8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de48bde8",
    "outputId": "5d3a7b26-51a9-43e2-965e-282fc108e7f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 437, 0, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB['yoyooyoyoy'], VOCAB['house'], VOCAB['<pad>'], VOCAB['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24247d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24247d56",
    "outputId": "9584a599-88b9-4842-9e4c-8c8193c4784a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13798\n"
     ]
    }
   ],
   "source": [
    "print(len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "574cce1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "574cce1b",
    "outputId": "851bcf05-5e08-4dd1-a550-c5669c9f1233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[437, 437, 4548, 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB(TOKENIZER(\"House house houses ThisisnotaKNownWord\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1e272",
   "metadata": {
    "id": "1be1e272"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df651a44",
   "metadata": {
    "id": "df651a44"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94741f76",
   "metadata": {
    "id": "94741f76"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab.vocab_factory import Vocab\n",
    "# Utility to transform text into a list of ints\n",
    "# This shoould go \"a b c\" -> [\"a\", \"b\", \"c\"] -> [1, 2, 3], for example\n",
    "def text_pipeline(x):\n",
    "    # Apply tokenizer to x\n",
    "    x = TOKENIZER(x)\n",
    "\n",
    "    # Return the Vocab at those tokens\n",
    "    return VOCAB(x)\n",
    "\n",
    "# Return a 0 starting version of x\n",
    "# If x = \"1\" this should return 0\n",
    "# If x = \"3\" this should return 2, Etc.\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95311731",
   "metadata": {
    "id": "95311731"
   },
   "outputs": [],
   "source": [
    "# For a batch of data that might not be a tensor, return the batch in ternsor version\n",
    "# batch is a length B lsit of tuples where each element is (label, text)\n",
    "# label is a raw string like \"1\" here; text is a sentence like \"this is about soccer\"\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (label, text) in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3} and append it to label list\n",
    "        label_list.append(label_pipeline(label))\n",
    "\n",
    "        # Return a list of ints\n",
    "        processed_text = torch.tensor(text_pipeline(text))\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "    # Make label_list into a tensor of dtype=torch.int64\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    # Pad the sequence\n",
    "    # For Exmaple: if we had 2 elements and [[1, 2], [1,2,3,4]] in the text_list then we want\n",
    "    # to have [[1, 2, 0, 0], [1, 2, 3, 4]] in text_list and text_list is a tensor\n",
    "    # Look up pad_sequence and make sure to specify batch_first=True and specify the padding_value=0\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Return the data and put it on a GPU or CPU, as needed\n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1292c44",
   "metadata": {
    "id": "b1292c44"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5da047d",
   "metadata": {
    "id": "b5da047d"
   },
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d2ae25e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d2ae25e",
    "outputId": "30452fc5-dee2-4036-8f27-e1f57ce6b488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4 ...\n"
     ]
    }
   ],
   "source": [
    "# Get an iterator for the AG_NEWS dataset and get the train version\n",
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "\n",
    "# Use the above to get the number of class elements\n",
    "num_class = len(set([label for (label, _) in train_iter]))\n",
    "# What are the classes?\n",
    "print(f\"The number of classes is {num_class} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52d408",
   "metadata": {
    "id": "fc52d408"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca51b36",
   "metadata": {
    "id": "eca51b36"
   },
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ab8cb7c",
   "metadata": {
    "id": "8ab8cb7c"
   },
   "outputs": [],
   "source": [
    "# A very naive model used to classify text\n",
    "class OneHotTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_class):\n",
    "        super(OneHotTextClassificationModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_class = num_class\n",
    "\n",
    "        # Have this layer take in data of dimension vocab_size and return data of dimension 100\n",
    "        # Don't use a bias\n",
    "        self.fc1 = nn.Linear(self.vocab_size, 100, bias=False)\n",
    "\n",
    "        # We will not use this, but see below as we want to mimic this layer using one_hot and fc1\n",
    "        self.e = nn.Embedding(vocab_size, 100)\n",
    "\n",
    "        # Have this layer take in 100 and return data of dimension num_class\n",
    "        # Don't use a bias\n",
    "        self.fc2 = nn.Linear(100, self.num_class, bias=False)\n",
    "        self.init_weights()\n",
    "\n",
    "        # See forward below\n",
    "        self.use_embedding_layer = False\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of fc1 to the same exact data as what self.e has\n",
    "        # Need to access the data within these layers\n",
    "        # Initialize the bias to zero\n",
    "        # Make sure to have the dimensions line up right\n",
    "        self.fc1.weight.data = self.e.weight.data.T\n",
    "\n",
    "        # Unitialize fc2 to uniform between -0.5 and 0.5\n",
    "        initrange = 0.5\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, K = x.shape\n",
    "        # x is of dimension (B, K), where K is the maximum number of tokens in an element of the batch\n",
    "        # Note: We will make this faster later on by using the nn.Embedding layer\n",
    "\n",
    "        # We will not use nn.Embedding, but the code below, a combination of F.one_hot and fc1, should be the SAME effect as the else clause\n",
    "        if not self.use_embedding_layer:\n",
    "          # Transform x to a tensor where each element is one-hot encoded\n",
    "          x = F.one_hot(x, self.vocab_size)\n",
    "          assert(x.shape == (B, K, self.vocab_size))\n",
    "\n",
    "          # Pass x through fc1 to get the row in fc1 correspondng to the row x is\n",
    "          x = self.fc1(x.float())\n",
    "          assert(x.shape == (B, K, 100))\n",
    "        else:\n",
    "          # Note: the above two steps should be the same as doing the command below\n",
    "          x = self.e(x)\n",
    "          assert(x.shape == (B, K, 100))\n",
    "\n",
    "        # Take the mean of the embedings for all words in each sentence\n",
    "        x = x.mean(dim=1)\n",
    "        assert(x.shape == (B, 100))\n",
    "\n",
    "        # Apply ReLU to x\n",
    "        x = nn.ReLU()(x)\n",
    "        assert(x.shape == (B, 100))\n",
    "\n",
    "        # Pass through fc2\n",
    "        x = self.fc2(x)\n",
    "        assert(x.shape == (B, self.num_class))\n",
    "\n",
    "        # Return the Logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a43d569e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a43d569e",
    "outputId": "62d2c736-bd12-4916-b2f3-6f893e5675c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7a50c0284db0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994ae19",
   "metadata": {
    "id": "8994ae19"
   },
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaaa82a2",
   "metadata": {
    "id": "eaaa82a2"
   },
   "outputs": [],
   "source": [
    "# Map the data to the right format\n",
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Split data into train and validation\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# Set up different DataLoaders\n",
    "train_dataloader = DataLoader(split_train_, collate_fn=collate_batch, batch_size=BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(split_valid_, collate_fn=collate_batch, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collate_batch, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5af374",
   "metadata": {
    "id": "7a5af374"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72b5bb91",
   "metadata": {
    "id": "72b5bb91"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d58cc1a9",
   "metadata": {
    "id": "d58cc1a9"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    # Put the model in train mode; this does not matter right now\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0.0\n",
    "    log_interval = 200\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get the predictions\n",
    "        predicted_label = model(text)\n",
    "\n",
    "        # Get the loss.\n",
    "        loss = loss_fn(input=predicted_label, target=label)\n",
    "\n",
    "        # The loss is computed by taking a mean, get the sum of the terms on the numerator\n",
    "        with torch.no_grad():\n",
    "          total_loss += loss.item() * label.size(0)\n",
    "\n",
    "        # Do back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to have max norm 0.1\n",
    "        # Look up torch.nn.utils.clip_grad_norm\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the accuracy\n",
    "        # predicted_label is (B, num_class) so take the argmax over the right dimension to get the actual label\n",
    "        total_acc += (torch.argmax(predicted_label, dim=1) == label).sum().item()\n",
    "\n",
    "        # Update the total number of items\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f} \"\n",
    "                \"| loss {:8.3f}\".format(\n",
    "                    epoch, idx,\n",
    "                    len(dataloader),\n",
    "                    total_acc/total_count,\n",
    "                    total_loss/total_count\n",
    "                    )\n",
    "            )\n",
    "            total_acc, total_count, total_loss = 0, 0, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85722617",
   "metadata": {
    "id": "85722617"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    # Put the model in eval model; this does not matter right now\n",
    "    model.eval()\n",
    "\n",
    "    # Set this to Accuracy from torchmetrics; use multiclass and specify the number of labels\n",
    "    accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_class)\n",
    "    total_acc = 0.0\n",
    "    total_count = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            # Get the predictions\n",
    "            predicted_label = model(text)\n",
    "            # Get the number of samples we have, the denominator of accuracy\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            # Get the total number of times we have the correct predictions, use accuracy_fn\n",
    "            total_acc += accuracy_fn(predicted_label, label).item() * label.size(0)\n",
    "\n",
    "            # Use accuracy_fn from torchmetrics to check that the total number of correct predictions is the same as if you use argmax on predicted_label\n",
    "            assert (\n",
    "                (torch.argmax(predicted_label, dim=1) == label).sum().item() == accuracy_fn(predicted_label, label).item() * label.size(0)\n",
    "            )\n",
    "\n",
    "    accuracy = total_acc / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W3LjZHTdrWW6",
   "metadata": {
    "id": "W3LjZHTdrWW6"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "This might take quite a bit of time to run since we use one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eTEl16pIBkTe",
   "metadata": {
    "id": "eTEl16pIBkTe"
   },
   "outputs": [],
   "source": [
    "# Set up the loss function\n",
    "# Note that this should be a multiclass classification problem and we take in logits\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Instantiate the model\n",
    "# Pass in the number of elements in VOCAB and num_class\n",
    "model = OneHotTextClassificationModel(len(VOCAB),num_class).to(DEVICE)\n",
    "\n",
    "# Instantiate the SGD optimizer with parameters LR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21ba24f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21ba24f3",
    "outputId": "0942ddae-156a-4499-b78f-6f160b9e7844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 7125 batches | accuracy    0.317 | loss    1.439\n",
      "| epoch   1 |   400/ 7125 batches | accuracy    0.413 | loss    1.277\n",
      "| epoch   1 |   600/ 7125 batches | accuracy    0.440 | loss    1.236\n",
      "| epoch   1 |   800/ 7125 batches | accuracy    0.485 | loss    1.188\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.522 | loss    1.117\n",
      "| epoch   1 |  1200/ 7125 batches | accuracy    0.536 | loss    1.124\n",
      "| epoch   1 |  1400/ 7125 batches | accuracy    0.544 | loss    1.086\n",
      "| epoch   1 |  1600/ 7125 batches | accuracy    0.574 | loss    1.041\n",
      "| epoch   1 |  1800/ 7125 batches | accuracy    0.557 | loss    1.045\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.585 | loss    0.995\n",
      "| epoch   1 |  2200/ 7125 batches | accuracy    0.600 | loss    0.987\n",
      "| epoch   1 |  2400/ 7125 batches | accuracy    0.613 | loss    0.968\n",
      "| epoch   1 |  2600/ 7125 batches | accuracy    0.625 | loss    0.958\n",
      "| epoch   1 |  2800/ 7125 batches | accuracy    0.627 | loss    0.913\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.634 | loss    0.920\n",
      "| epoch   1 |  3200/ 7125 batches | accuracy    0.640 | loss    0.889\n",
      "| epoch   1 |  3400/ 7125 batches | accuracy    0.652 | loss    0.902\n",
      "| epoch   1 |  3600/ 7125 batches | accuracy    0.642 | loss    0.910\n",
      "| epoch   1 |  3800/ 7125 batches | accuracy    0.659 | loss    0.887\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.658 | loss    0.858\n",
      "| epoch   1 |  4200/ 7125 batches | accuracy    0.669 | loss    0.857\n",
      "| epoch   1 |  4400/ 7125 batches | accuracy    0.684 | loss    0.833\n",
      "| epoch   1 |  4600/ 7125 batches | accuracy    0.696 | loss    0.826\n",
      "| epoch   1 |  4800/ 7125 batches | accuracy    0.693 | loss    0.804\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.701 | loss    0.796\n",
      "| epoch   1 |  5200/ 7125 batches | accuracy    0.697 | loss    0.779\n",
      "| epoch   1 |  5400/ 7125 batches | accuracy    0.713 | loss    0.799\n",
      "| epoch   1 |  5600/ 7125 batches | accuracy    0.696 | loss    0.775\n",
      "| epoch   1 |  5800/ 7125 batches | accuracy    0.694 | loss    0.774\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.697 | loss    0.782\n",
      "| epoch   1 |  6200/ 7125 batches | accuracy    0.715 | loss    0.736\n",
      "| epoch   1 |  6400/ 7125 batches | accuracy    0.706 | loss    0.748\n",
      "| epoch   1 |  6600/ 7125 batches | accuracy    0.731 | loss    0.713\n",
      "| epoch   1 |  6800/ 7125 batches | accuracy    0.715 | loss    0.746\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.722 | loss    0.725\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 2028.72s | valid accuracy    0.694 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 7125 batches | accuracy    0.732 | loss    0.709\n",
      "| epoch   2 |   400/ 7125 batches | accuracy    0.733 | loss    0.712\n",
      "| epoch   2 |   600/ 7125 batches | accuracy    0.721 | loss    0.725\n",
      "| epoch   2 |   800/ 7125 batches | accuracy    0.726 | loss    0.706\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.748 | loss    0.678\n",
      "| epoch   2 |  1200/ 7125 batches | accuracy    0.737 | loss    0.696\n",
      "| epoch   2 |  1400/ 7125 batches | accuracy    0.737 | loss    0.704\n",
      "| epoch   2 |  1600/ 7125 batches | accuracy    0.754 | loss    0.679\n",
      "| epoch   2 |  1800/ 7125 batches | accuracy    0.724 | loss    0.704\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.743 | loss    0.678\n",
      "| epoch   2 |  2200/ 7125 batches | accuracy    0.751 | loss    0.672\n",
      "| epoch   2 |  2400/ 7125 batches | accuracy    0.753 | loss    0.663\n",
      "| epoch   2 |  2600/ 7125 batches | accuracy    0.745 | loss    0.676\n",
      "| epoch   2 |  2800/ 7125 batches | accuracy    0.753 | loss    0.655\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.757 | loss    0.669\n",
      "| epoch   2 |  3200/ 7125 batches | accuracy    0.758 | loss    0.651\n",
      "| epoch   2 |  3400/ 7125 batches | accuracy    0.760 | loss    0.658\n",
      "| epoch   2 |  3600/ 7125 batches | accuracy    0.739 | loss    0.699\n",
      "| epoch   2 |  3800/ 7125 batches | accuracy    0.758 | loss    0.688\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.751 | loss    0.656\n",
      "| epoch   2 |  4200/ 7125 batches | accuracy    0.747 | loss    0.684\n",
      "| epoch   2 |  4400/ 7125 batches | accuracy    0.771 | loss    0.637\n",
      "| epoch   2 |  4600/ 7125 batches | accuracy    0.772 | loss    0.637\n",
      "| epoch   2 |  4800/ 7125 batches | accuracy    0.769 | loss    0.620\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.777 | loss    0.633\n",
      "| epoch   2 |  5200/ 7125 batches | accuracy    0.763 | loss    0.626\n",
      "| epoch   2 |  5400/ 7125 batches | accuracy    0.776 | loss    0.638\n",
      "| epoch   2 |  5600/ 7125 batches | accuracy    0.768 | loss    0.624\n",
      "| epoch   2 |  5800/ 7125 batches | accuracy    0.766 | loss    0.620\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.762 | loss    0.632\n",
      "| epoch   2 |  6200/ 7125 batches | accuracy    0.782 | loss    0.596\n",
      "| epoch   2 |  6400/ 7125 batches | accuracy    0.766 | loss    0.615\n",
      "| epoch   2 |  6600/ 7125 batches | accuracy    0.788 | loss    0.587\n",
      "| epoch   2 |  6800/ 7125 batches | accuracy    0.775 | loss    0.623\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.779 | loss    0.598\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 2035.06s | valid accuracy    0.757 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 7125 batches | accuracy    0.785 | loss    0.596\n",
      "| epoch   3 |   400/ 7125 batches | accuracy    0.779 | loss    0.594\n",
      "| epoch   3 |   600/ 7125 batches | accuracy    0.773 | loss    0.594\n",
      "| epoch   3 |   800/ 7125 batches | accuracy    0.782 | loss    0.595\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.792 | loss    0.583\n",
      "| epoch   3 |  1200/ 7125 batches | accuracy    0.783 | loss    0.580\n",
      "| epoch   3 |  1400/ 7125 batches | accuracy    0.786 | loss    0.603\n",
      "| epoch   3 |  1600/ 7125 batches | accuracy    0.801 | loss    0.574\n",
      "| epoch   3 |  1800/ 7125 batches | accuracy    0.772 | loss    0.606\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.786 | loss    0.581\n",
      "| epoch   3 |  2200/ 7125 batches | accuracy    0.791 | loss    0.572\n",
      "| epoch   3 |  2400/ 7125 batches | accuracy    0.797 | loss    0.562\n",
      "| epoch   3 |  2600/ 7125 batches | accuracy    0.781 | loss    0.588\n",
      "| epoch   3 |  2800/ 7125 batches | accuracy    0.799 | loss    0.555\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.795 | loss    0.592\n",
      "| epoch   3 |  3200/ 7125 batches | accuracy    0.794 | loss    0.561\n",
      "| epoch   3 |  3400/ 7125 batches | accuracy    0.795 | loss    0.560\n",
      "| epoch   3 |  3600/ 7125 batches | accuracy    0.773 | loss    0.614\n",
      "| epoch   3 |  3800/ 7125 batches | accuracy    0.792 | loss    0.606\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.784 | loss    0.566\n",
      "| epoch   3 |  4200/ 7125 batches | accuracy    0.788 | loss    0.599\n",
      "| epoch   3 |  4400/ 7125 batches | accuracy    0.804 | loss    0.551\n",
      "| epoch   3 |  4600/ 7125 batches | accuracy    0.805 | loss    0.567\n",
      "| epoch   3 |  4800/ 7125 batches | accuracy    0.791 | loss    0.554\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.805 | loss    0.562\n",
      "| epoch   3 |  5200/ 7125 batches | accuracy    0.795 | loss    0.553\n",
      "| epoch   3 |  5400/ 7125 batches | accuracy    0.802 | loss    0.566\n",
      "| epoch   3 |  5600/ 7125 batches | accuracy    0.795 | loss    0.556\n",
      "| epoch   3 |  5800/ 7125 batches | accuracy    0.798 | loss    0.549\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.798 | loss    0.566\n",
      "| epoch   3 |  6200/ 7125 batches | accuracy    0.811 | loss    0.524\n",
      "| epoch   3 |  6400/ 7125 batches | accuracy    0.793 | loss    0.543\n",
      "| epoch   3 |  6600/ 7125 batches | accuracy    0.815 | loss    0.526\n",
      "| epoch   3 |  6800/ 7125 batches | accuracy    0.799 | loss    0.556\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.803 | loss    0.525\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 2043.23s | valid accuracy    0.795 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 7125 batches | accuracy    0.803 | loss    0.537\n",
      "| epoch   4 |   400/ 7125 batches | accuracy    0.801 | loss    0.529\n",
      "| epoch   4 |   600/ 7125 batches | accuracy    0.800 | loss    0.531\n",
      "| epoch   4 |   800/ 7125 batches | accuracy    0.802 | loss    0.536\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.815 | loss    0.527\n",
      "| epoch   4 |  1200/ 7125 batches | accuracy    0.813 | loss    0.513\n",
      "| epoch   4 |  1400/ 7125 batches | accuracy    0.804 | loss    0.544\n",
      "| epoch   4 |  1600/ 7125 batches | accuracy    0.818 | loss    0.527\n",
      "| epoch   4 |  1800/ 7125 batches | accuracy    0.791 | loss    0.561\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.802 | loss    0.536\n",
      "| epoch   4 |  2200/ 7125 batches | accuracy    0.815 | loss    0.521\n",
      "| epoch   4 |  2400/ 7125 batches | accuracy    0.817 | loss    0.515\n",
      "| epoch   4 |  2600/ 7125 batches | accuracy    0.810 | loss    0.538\n",
      "| epoch   4 |  2800/ 7125 batches | accuracy    0.822 | loss    0.502\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.811 | loss    0.538\n",
      "| epoch   4 |  3200/ 7125 batches | accuracy    0.818 | loss    0.518\n",
      "| epoch   4 |  3400/ 7125 batches | accuracy    0.819 | loss    0.502\n",
      "| epoch   4 |  3600/ 7125 batches | accuracy    0.797 | loss    0.557\n",
      "| epoch   4 |  3800/ 7125 batches | accuracy    0.808 | loss    0.555\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.815 | loss    0.524\n",
      "| epoch   4 |  4200/ 7125 batches | accuracy    0.803 | loss    0.562\n",
      "| epoch   4 |  4400/ 7125 batches | accuracy    0.825 | loss    0.504\n",
      "| epoch   4 |  4600/ 7125 batches | accuracy    0.828 | loss    0.511\n",
      "| epoch   4 |  4800/ 7125 batches | accuracy    0.805 | loss    0.524\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.817 | loss    0.522\n",
      "| epoch   4 |  5200/ 7125 batches | accuracy    0.817 | loss    0.513\n",
      "| epoch   4 |  5400/ 7125 batches | accuracy    0.823 | loss    0.511\n",
      "| epoch   4 |  5600/ 7125 batches | accuracy    0.816 | loss    0.523\n",
      "| epoch   4 |  5800/ 7125 batches | accuracy    0.817 | loss    0.510\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.818 | loss    0.516\n",
      "| epoch   4 |  6200/ 7125 batches | accuracy    0.821 | loss    0.499\n",
      "| epoch   4 |  6400/ 7125 batches | accuracy    0.813 | loss    0.496\n",
      "| epoch   4 |  6600/ 7125 batches | accuracy    0.834 | loss    0.485\n",
      "| epoch   4 |  6800/ 7125 batches | accuracy    0.817 | loss    0.516\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.824 | loss    0.490\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 2038.61s | valid accuracy    0.849 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 7125 batches | accuracy    0.823 | loss    0.495\n",
      "| epoch   5 |   400/ 7125 batches | accuracy    0.822 | loss    0.493\n",
      "| epoch   5 |   600/ 7125 batches | accuracy    0.817 | loss    0.488\n",
      "| epoch   5 |   800/ 7125 batches | accuracy    0.821 | loss    0.503\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.825 | loss    0.496\n",
      "| epoch   5 |  1200/ 7125 batches | accuracy    0.826 | loss    0.478\n",
      "| epoch   5 |  1400/ 7125 batches | accuracy    0.822 | loss    0.502\n",
      "| epoch   5 |  1600/ 7125 batches | accuracy    0.833 | loss    0.494\n",
      "| epoch   5 |  1800/ 7125 batches | accuracy    0.809 | loss    0.528\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.824 | loss    0.495\n",
      "| epoch   5 |  2200/ 7125 batches | accuracy    0.830 | loss    0.484\n",
      "| epoch   5 |  2400/ 7125 batches | accuracy    0.829 | loss    0.485\n",
      "| epoch   5 |  2600/ 7125 batches | accuracy    0.826 | loss    0.505\n",
      "| epoch   5 |  2800/ 7125 batches | accuracy    0.839 | loss    0.466\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.823 | loss    0.507\n",
      "| epoch   5 |  3200/ 7125 batches | accuracy    0.827 | loss    0.483\n",
      "| epoch   5 |  3400/ 7125 batches | accuracy    0.837 | loss    0.467\n",
      "| epoch   5 |  3600/ 7125 batches | accuracy    0.813 | loss    0.529\n",
      "| epoch   5 |  3800/ 7125 batches | accuracy    0.822 | loss    0.514\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.818 | loss    0.503\n",
      "| epoch   5 |  4200/ 7125 batches | accuracy    0.819 | loss    0.528\n",
      "| epoch   5 |  4400/ 7125 batches | accuracy    0.841 | loss    0.461\n",
      "| epoch   5 |  4600/ 7125 batches | accuracy    0.833 | loss    0.489\n",
      "| epoch   5 |  4800/ 7125 batches | accuracy    0.826 | loss    0.482\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.829 | loss    0.499\n",
      "| epoch   5 |  5200/ 7125 batches | accuracy    0.825 | loss    0.487\n",
      "| epoch   5 |  5400/ 7125 batches | accuracy    0.845 | loss    0.471\n",
      "| epoch   5 |  5600/ 7125 batches | accuracy    0.828 | loss    0.485\n",
      "| epoch   5 |  5800/ 7125 batches | accuracy    0.831 | loss    0.479\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.837 | loss    0.482\n",
      "| epoch   5 |  6200/ 7125 batches | accuracy    0.838 | loss    0.461\n",
      "| epoch   5 |  6400/ 7125 batches | accuracy    0.823 | loss    0.475\n",
      "| epoch   5 |  6600/ 7125 batches | accuracy    0.846 | loss    0.446\n",
      "| epoch   5 |  6800/ 7125 batches | accuracy    0.831 | loss    0.487\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.834 | loss    0.454\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 2044.04s | valid accuracy    0.832 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.827\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, loss_fn, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            accu_val\n",
    "            )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71904d5f",
   "metadata": {
    "id": "71904d5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
